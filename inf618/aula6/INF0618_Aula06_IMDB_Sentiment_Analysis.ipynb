{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of IMDB movie reviews\n",
    "\n",
    "In this exercise, we will try to classify whether a IMDB review made on a movie can be considered as positive or negative. To do that, we will use a recurrent network with a LSTM module inside it.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/site-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.23) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Movie Reviews Dataset\n",
    "[The dataset we will use](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification) comprises 50,000  movies reviews from IMDB (25k for training and 25k for testing), labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data.\n",
    "\n",
    "A common approach in Natural Language Processing (NLP) is to use a dictionary/vocabulary to encode the words present in the text being processed. There are different ways of building a dictionary, but essentially we hope that it will comprise the most significant words in all your training data (assuming it will generalize well for the testing set). **In this exercise, our dictionary will be composed with the 20,000 most frequent words in our training set.** Each word in a movie review will be encoded (transformed) to a integer associated with a word in our dictionary/vocabulary, indexed by frequency (most frequent words will receive lowest integers). \n",
    "\n",
    "For example, suppose our dictionary is: \n",
    "\n",
    "`{\"movie\":1, \"actor\": 2, \"actress\": 3, \"cool\":4, \"bad\":5, \"action\":6 ... \"awesome\": 100 ...}`\n",
    "\n",
    "Associating the word `movie` (the most common word in the training set) to the number `1`, `bad` to the number `5` and so on. Now, supose we have the following two reviews (disconsidering words that are not in the vocabulary):\n",
    "\n",
    "> **Review 1:** \"movie awesome. Cool actor.\"\n",
    ">\n",
    "> **Review 2:** \"movie bad. Awesome actor.\"\n",
    "\n",
    "They will be encoded as:\n",
    "\n",
    "> **Encode 1:** [1,100,4,2] \n",
    ">\n",
    "> **Encode 2:** [1,5,100,2]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences: \t 12500  positives \t 12500  negatives\n",
      "25000 val sequences: \t 12500  positives \t 12500  negatives\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 20000 #The size of our vocabulary/dictionary is the 20k most frequent words\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_val, y_val) = imdb.load_data(num_words = vocabulary_size)\n",
    "\n",
    "print(len(x_train), 'train sequences: \\t', sum(y_train == 1), \" positives \\t\", sum(y_train == 0), \" negatives\")\n",
    "print(len(x_val), 'val sequences: \\t', sum(y_val == 1), \" positives \\t\", sum(y_val == 0), \" negatives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine one example of review and encoded array: \n",
    "\n",
    "(Just change `review_idx` to another number to see other reviews.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This downloads the \"reverse dictionary\", the mapping of word and index\n",
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW:\n",
      " <START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all \n",
      "\n",
      "ENCODED:\n",
      " [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32] \n",
      "\n",
      "CLASS (0 = negative, 1 = positive):  1\n"
     ]
    }
   ],
   "source": [
    "review_idx = 0\n",
    "print(\"REVIEW:\\n\", ' '.join(id_to_word[id] for id in x_train[review_idx]), \"\\n\")\n",
    "print(\"ENCODED:\\n\", x_train[review_idx],\"\\n\")\n",
    "print(\"CLASS (0 = negative, 1 = positive): \", y_train[review_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each review also has a variable number of words in it, so before processing them, we will make sure they all have the same length. We will limit the number of words in each review to 80.\n",
    "\n",
    "On the other hand, those that have less than 80 words will be padded to have length = 80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x timesteps/words)\n",
      "x_train shape: (25000, 80)\n",
      "x_val shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "maxlen = 80  # cut texts after this number of words (among the top vocabulary_size most common words)\n",
    "\n",
    "print('Pad sequences (samples x timesteps/words)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_val shape:', x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "We will define a simple model composed of:\n",
    "- [Embedding layer](https://keras.io/layers/embeddings/) mapping our vocabulary size to features of 128 dimensions;\n",
    "- [LSTM layer](https://keras.io/layers/recurrent/#lstm) with 128 units, with 40% dropout (both `dropout` and `recurrent_dropout`) \n",
    "- [Dense layer](https://keras.io/layers/core/#dense) with 1 neuron (because it is a binary problem) and sigmoid activation.\n",
    "\n",
    "Besides that, we will use `Adam` optimizer and `binary_crossentropy` loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, 128))\n",
    "model.add(LSTM(128, dropout=0.4, recurrent_dropout=0.4))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation\n",
    "Now let's train our model and monitor the loss and accuracy in the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 82s 3ms/step - loss: 0.4987 - acc: 0.7566 - val_loss: 0.4393 - val_acc: 0.7985\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 77s 3ms/step - loss: 0.3573 - acc: 0.8495 - val_loss: 0.3814 - val_acc: 0.8298\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 81s 3ms/step - loss: 0.2802 - acc: 0.8868 - val_loss: 0.4006 - val_acc: 0.8264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f73cf63bef0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "model.fit(x_train, y_train, \n",
    "          batch_size=batch_size, epochs=3, \n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to train your model for more epochs, but you should probably be carefull with overfitting. As we can see, our training loss keeps increasing while the validation loss isn't. Try adding more regularization to the model to deal with that.\n",
    "\n",
    "Let's see some of the wrongly classified reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 15s 587us/step\n"
     ]
    }
   ],
   "source": [
    "predicted_classes = [pred[0] for pred in model.predict_classes(x_val, verbose=1)]\n",
    "\n",
    "# Check which items we got wrong\n",
    "incorrect_indices = np.nonzero(predicted_classes != y_val)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS =  1\n",
      "REVIEW:\n",
      " poor it keeps shaking all the time in a completely tasteless framing br br its really painful to see this very interesting film in a cinema you got quickly <UNK> and you have to make some huge effort not to puke on your neighbor 's seat br br it's really a shame <UNK> the story is edited in a non linear way which is quite rare and a very good idea for a documentary br br watch this at home\n",
      "\n",
      "\n",
      "CLASS =  0\n",
      "REVIEW:\n",
      " the <UNK> this sequence is shown repeatedly from various angles thus drawing out what probably was only a five second event br br <UNK> is a film that the revolutionary spirit celebrates it for those already committed and it for the <UNK> it <UNK> of fire and <UNK> with the senseless injustices of the decadent <UNK> regime its greatest impact has been on film students who have borrowed and only slightly improved on techniques invented in russia several generations ago\n"
     ]
    }
   ],
   "source": [
    "posIdx = [idx for idx in incorrect_indices if predicted_classes[idx] == 1]\n",
    "negIdx = [idx for idx in incorrect_indices if predicted_classes[idx] == 0]\n",
    "\n",
    "#Select a posIdx or negIdx\n",
    "idx = posIdx[0]\n",
    "print(\"CLASS = \", predicted_classes[idx])\n",
    "print(\"REVIEW:\\n\", ' '.join(id_to_word[id] for id in x_val[idx]))\n",
    "\n",
    "idx = negIdx[0]\n",
    "print(\"\\n\\nCLASS = \", predicted_classes[idx])\n",
    "print(\"REVIEW:\\n\", ' '.join(id_to_word[id] for id in x_val[idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
