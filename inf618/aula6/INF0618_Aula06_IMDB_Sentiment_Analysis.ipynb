{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of IMDB movie reviews\n",
    "\n",
    "In this exercise, we will try to classify whether a IMDB review made on a movie can be considered as positive or negative. To do that, we will use a recurrent network with a LSTM module inside it.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Movie Reviews Dataset\n",
    "[The dataset we will use](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification) comprises 50,000  movies reviews from IMDB (25k for training and 25k for testing), labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data.\n",
    "\n",
    "A common approach in Natural Language Processing (NLP) is to use a dictionary/vocabulary to encode the words present in the text being processed. There are different ways of building a dictionary, but essentially we hope that it will comprise the most significant words in all your training data (assuming it will generalize well for the testing set). **In this exercise, our dictionary will be composed with the 20,000 most frequent words in our training set.** Each word in a movie review will be encoded (transformed) to a integer associated with a word in our dictionary/vocabulary, indexed by frequency (most frequent words will receive lowest integers). \n",
    "\n",
    "For example, suppose our dictionary is: \n",
    "\n",
    "`{\"movie\":1, \"actor\": 2, \"actress\": 3, \"cool\":4, \"bad\":5, \"action\":6 ... \"awesome\": 100 ...}`\n",
    "\n",
    "Associating the word `movie` (the most common word in the training set) to the number `1`, `bad` to the number `5` and so on. Now, supose we have the following two reviews (disconsidering words that are not in the vocabulary):\n",
    "\n",
    "> **Review 1:** \"movie awesome. Cool actor.\"\n",
    ">\n",
    "> **Review 2:** \"movie bad. Awesome actor.\"\n",
    "\n",
    "They will be encoded as:\n",
    "\n",
    "> **Encode 1:** [1,100,4,2] \n",
    ">\n",
    "> **Encode 2:** [1,5,100,2]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 20000 #The size of our vocabulary/dictionary is the 20k most frequent words\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_val, y_val) = imdb.load_data(num_words = vocabulary_size)\n",
    "\n",
    "print(len(x_train), 'train sequences: \\t', sum(y_train == 1), \" positives \\t\", sum(y_train == 0), \" negatives\")\n",
    "print(len(x_val), 'val sequences: \\t', sum(y_val == 1), \" positives \\t\", sum(y_val == 0), \" negatives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine one example of review and encoded array: \n",
    "\n",
    "(Just change `review_idx` to another number to see other reviews.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This downloads the \"reverse dictionary\", the mapping of word and index\n",
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_idx = 0\n",
    "print(\"REVIEW:\\n\", ' '.join(id_to_word[id] for id in x_train[review_idx]), \"\\n\")\n",
    "print(\"ENCODED:\\n\", x_train[review_idx],\"\\n\")\n",
    "print(\"CLASS (0 = negative, 1 = positive): \", y_train[review_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each review also has a variable number of words in it, so before processing them, we will make sure they all have the same length. We will limit the number of words in each review to 80.\n",
    "\n",
    "On the other hand, those that have less than 80 words will be padded to have length = 80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 80  # cut texts after this number of words (among the top vocabulary_size most common words)\n",
    "\n",
    "print('Pad sequences (samples x timesteps/words)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_val shape:', x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "We will define a simple model composed of:\n",
    "- [Embedding layer](https://keras.io/layers/embeddings/) mapping our vocabulary size to features of 128 dimensions;\n",
    "- [LSTM layer](https://keras.io/layers/recurrent/#lstm) with 128 units, with 40% dropout (both `dropout` and `recurrent_dropout`) \n",
    "- [Dense layer](https://keras.io/layers/core/#dense) with 1 neuron (because it is a binary problem) and sigmoid activation.\n",
    "\n",
    "Besides that, we will use `Adam` optimizer and `binary_crossentropy` loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, 128))\n",
    "model.add(LSTM(128, dropout=0.4, recurrent_dropout=0.4))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation\n",
    "Now let's train our model and monitor the loss and accuracy in the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "model.fit(x_train, y_train, \n",
    "          batch_size=batch_size, epochs=3, \n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to train your model for more epochs, but you should probably be carefull with overfitting. As we can see, our training loss keeps increasing while the validation loss isn't. Try adding more regularization to the model to deal with that.\n",
    "\n",
    "Let's see some of the wrongly classified reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = [pred[0] for pred in model.predict_classes(x_val, verbose=1)]\n",
    "\n",
    "# Check which items we got wrong\n",
    "incorrect_indices = np.nonzero(predicted_classes != y_val)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posIdx = [idx for idx in incorrect_indices if predicted_classes[idx] == 1]\n",
    "negIdx = [idx for idx in incorrect_indices if predicted_classes[idx] == 0]\n",
    "\n",
    "#Select a posIdx or negIdx\n",
    "idx = posIdx[0]\n",
    "print(\"CLASS = \", predicted_classes[idx])\n",
    "print(\"REVIEW:\\n\", ' '.join(id_to_word[id] for id in x_val[idx]))\n",
    "\n",
    "idx = negIdx[0]\n",
    "print(\"\\n\\nCLASS = \", predicted_classes[idx])\n",
    "print(\"REVIEW:\\n\", ' '.join(id_to_word[id] for id in x_val[idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
