{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network with Keras\n",
    "\n",
    "In this exercise, we will build a convolutional neural network (CNN) to classify the handwritten digits from MNIST. The initial parts (imports and load) are the same as what we did on the NN exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "First let's import some prerequisites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle, seed\n",
    "seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (15,15) # Make the figures a bit bigger\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 10\n",
    "\n",
    "# the data, shuffled and split between trainVal and test sets\n",
    "(trainVal_data, trainVal_label), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# We want now to split the trainVal data into train and validation sets\n",
    "nData = trainVal_data.shape[0]  #find the size of trainVal\n",
    "nTrain = int(nData * 0.8)  #80% to train, 20% to val\n",
    "\n",
    "randomIdx = list(range(nData))   #randomly select indexes\n",
    "shuffle(randomIdx)\n",
    "trainIdx = randomIdx[:nTrain] \n",
    "valIdx = randomIdx[nTrain:]\n",
    "\n",
    "# Split the data\n",
    "X_val, y_val = trainVal_data[valIdx], trainVal_label[valIdx]\n",
    "X_train, y_train = trainVal_data[trainIdx], trainVal_label[trainIdx]\n",
    "\n",
    "print(\"X_train original shape\", X_train.shape)\n",
    "print(\"y_train original shape\", y_train.shape), \"\\n\"\n",
    "\n",
    "print(\"X_val original shape\", X_val.shape)\n",
    "print(\"y_val original shape\", y_val.shape), \"\\n\"\n",
    "\n",
    "print(\"X_test original shape\", X_test.shape)\n",
    "print(\"y_test original shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some examples of the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_train[i], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Class {}\".format(y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How balanced is our training?\n",
    "Let's check how many examples we have for each class and calculate a class weight. This can be useful to tell the model to \"pay more attention\" during training to samples from an under-represented class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"Train ---> \", dict(zip(unique, counts)), \"\\n\")\n",
    "\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "train_class_weights = dict(enumerate(class_weights))\n",
    "print(train_class_weights, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the test set ---> We will use this to find the normalized accuracy\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "print(\"Test ---> \", dict(zip(unique, counts)), \"\\n\")\n",
    "\n",
    "test_sample_per_class = counts\n",
    "print(test_sample_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data for training\n",
    "Differently to what we did in our neural network, the CNN operates on images as input, so it is not necessary to transform the image to an array. However we need to inform to our network the shape of our images. Colored images usually have 3 channels (RGB), but we are working with grayscale images which have only a single channel. **An important point to notice**, Tensorflow assumes the number of channels are always as the last dimension of the image, while Torch assumes it is in the first dimension:\n",
    "\n",
    "```\n",
    "Tensorflow -> (img_rows, img_cols, channels)\n",
    "Torch -> (channels, img_rows, img_cols)\n",
    "```\n",
    "\n",
    "Besides that, just like we did before, we are going to scale the inputs to be in the range [0-1] rather than [0-255]. **Other types of preprocessing, such as scaling, normalization and cropping could also be done in this part.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "#The first dimension refers to the number of images\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_val = X_val.reshape(X_val.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "X_val /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(\"Training matrix shape\", X_train.shape)\n",
    "print(\"Validation matrix shape\", X_val.shape)\n",
    "print(\"Testing matrix shape\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also modify the target matrices to be in the one-hot format, i.e.\n",
    "\n",
    "```\n",
    "0 -> [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "1 -> [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "2 -> [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "3 -> [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "...\n",
    "9 -> [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "```\n",
    "\n",
    "Keras already has a method to do this, `np_utils.to_categorical`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_val = np_utils.to_categorical(y_val, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the architecture\n",
    "We will define a simple CNN with the layers that we have learned so far. Our network will have:\n",
    "- Convolutional layer with 10 filters of size 5x5, followed by ReLU activation;\n",
    "- Max pooling layer with kernel 2x2, that will reduce each spatial dimension by half (24x24 to 12x12);\n",
    "- An operation to flatten the feature maps into an array of size 10x12x12 = 1440\n",
    "- Dropout operation with probability 0.25, applied to flattened array\n",
    "- Fully connected layer with units/neurons equal to the number of classes in our problem (in this case, 10);\n",
    "- Softmax activation on the last FC layer.\n",
    "\n",
    "The architecture will look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cnn_mnist.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It is important to notice** that most layers allow us to define the activation (ReLU, sigmoid, softmax) by passing a string to the `activation` argument. On the Neural Network exercise, we defined it by doing a `model.add(Activation('sigmoid'))`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model\n",
    "Let's define the loss and optimizer we will use to train our CNN, as well as specify that we want to record the accuracy in each epoch. We will use the categorial cross-entropy as our loss and the SGD optimizer. **Remember that Keras does not output the normalized accuracy!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.01) #lr = learning rate\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "#Similar to the layers, we can define the optimizer with a string 'sgd'\n",
    "# which will give us the SGD optimizer with default parameters\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model!\n",
    "We are ready to train our CNN! Just like in the NN, the `model.fit` method receives the same arguments:\n",
    "- **X_train & Y_train:** the training data and labels;\n",
    "- **batch_size:** instead of optimizing your network on all training data, we usually optimize it over batches (small samples from the training data). In this case, we will update our weights (backpropagation) after forwarding/processing 128 images;\n",
    "- **nb_epoch:** an epoch occurs when all your training data is processed/forwarded once. As CNNs are more computationally expensive than NNs, we will only process 1 epoch.\n",
    "- **verbose:** show loss after each epoch\n",
    "- **class_weight:** the weight of each class, considering they are not balanced. This will ponder the loss based on how frequent each class is;\n",
    "- **validation_data:** after each epoch our model will predict these data and output the loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "          batch_size=128, epochs=1, verbose=1,\n",
    "          class_weight = train_class_weights,\n",
    "          validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, evaluate its performance on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy (NOT NORMALIZED):', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = model.predict_classes(X_test)\n",
    "Y_test_classes = np.argmax(Y_test, axis=-1)\n",
    "\n",
    "accPerClass = []\n",
    "for classIdx in range(nb_classes):\n",
    "    idx = (Y_test_classes == classIdx)\n",
    "    \n",
    "    correctPred = np.sum(predicted_classes[idx] == Y_test_classes[idx])\n",
    "    accPerClass.append( correctPred / float(test_sample_per_class[classIdx]))\n",
    "    \n",
    "print(dict(zip(range(nb_classes),accPerClass)), \"\\n\")\n",
    "print (\"Normalized Acc --> \", np.mean(accPerClass))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the output\n",
    "\n",
    "It's always a good idea to inspect the output and make sure everything looks sane. Here we'll look at some examples it gets right, and some examples it gets wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predict_classes function outputs the highest probability class\n",
    "# according to the trained classifier for each input example.\n",
    "predicted_classes = model.predict_classes(X_test)\n",
    "\n",
    "# Check which items we got right / wrong\n",
    "correct_indices = np.nonzero(predicted_classes == y_test)[0]\n",
    "incorrect_indices = np.nonzero(predicted_classes != y_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i, correct in enumerate(correct_indices[:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_test[correct].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct], y_test[correct]))\n",
    "    \n",
    "plt.figure()\n",
    "for i, incorrect in enumerate(incorrect_indices[:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[incorrect], y_test[incorrect]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This notebook was heavily inspired in [Keras tutorial on CNN](https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py);\n",
    "- The image of the CNN architecture was adapted from [here](https://codetolight.wordpress.com/2017/11/29/getting-started-with-pytorch-for-deep-learning-part-3-neural-network-basics/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
