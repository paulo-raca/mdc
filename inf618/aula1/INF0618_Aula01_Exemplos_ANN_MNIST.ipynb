{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a simple neural-network with Keras\n",
    "\n",
    "Written by **Xavier Snelgrove** ([link to his GitHub](https://github.com/wxs/keras-mnist-tutorial/blob/master/MNIST%20in%20Keras.ipynb)), with minor modifications from **Rafael Padilha**.\n",
    "\n",
    "This is a simple tutorial on working with neural networks in Keras. In this exercise, we will define, train and evaluate a simple feed-forward NN to classify handwritten digits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "First let's import some prerequisites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle, seed\n",
    "seed(42)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (15,15) # Make the figures a bit bigger\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 10\n",
    "\n",
    "# the data, shuffled and split between trainVal and test sets\n",
    "(trainVal_data, trainVal_label), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# We want now to split the trainVal data into train and validation sets\n",
    "nData = trainVal_data.shape[0]  #find the size of trainVal\n",
    "nTrain = int(nData * 0.8)  #80% to train, 20% to val\n",
    "\n",
    "randomIdx = list(range(nData))   #randomly select indexes\n",
    "shuffle(randomIdx)\n",
    "trainIdx = randomIdx[:nTrain] \n",
    "valIdx = randomIdx[nTrain:]\n",
    "\n",
    "# Split the data\n",
    "X_val, y_val = trainVal_data[valIdx], trainVal_label[valIdx]\n",
    "X_train, y_train = trainVal_data[trainIdx], trainVal_label[trainIdx]\n",
    "\n",
    "print(\"X_train original shape\", X_train.shape)\n",
    "print(\"y_train original shape\", y_train.shape), \"\\n\"\n",
    "\n",
    "print(\"X_val original shape\", X_val.shape)\n",
    "print(\"y_val original shape\", y_val.shape), \"\\n\"\n",
    "\n",
    "print(\"X_test original shape\", X_test.shape)\n",
    "print(\"y_test original shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some examples of the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_train[i], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Class {}\".format(y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How balanced is our training?\n",
    "Let's check how many examples we have for each class and calculate a class weight. This can be useful to tell the model to \"pay more attention\" during training to samples from an under-represented class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print (\"Train ---> \", dict(zip(unique, counts)), \"\\n\")\n",
    "\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "train_class_weights = dict(enumerate(class_weights))\n",
    "print (train_class_weights, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the test set ---> We will use this to find the normalized accuracy\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "print(\"Test ---> \", dict(zip(unique, counts)), \"\\n\")\n",
    "\n",
    "test_sample_per_class = counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format the data for training\n",
    "Our neural-network is going to take a single vector for each training example, so we need to reshape the input so that each 28x28 image becomes a single 784 dimensional vector. We'll also scale the inputs to be in the range [0-1] rather than [0-255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(48000, 784)\n",
    "X_val = X_val.reshape(12000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "X_val /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(\"Training matrix shape\", X_train.shape)\n",
    "print(\"Validation matrix shape\", X_val.shape)\n",
    "print(\"Testing matrix shape\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also modify the target matrices to be in the one-hot format, i.e.\n",
    "\n",
    "```\n",
    "0 -> [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "1 -> [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "2 -> [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "3 -> [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "...\n",
    "9 -> [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "```\n",
    "\n",
    "Keras already has a method to do this, `np_utils.to_categorical`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_val = np_utils.to_categorical(y_val, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the neural network\n",
    "Build the neural-network. Here we'll do a simple neural network with 4 layers (input + 2 hidden layers + output), using fully connected layers. In this type of layer, each neuron is connected to every input units. If we consider the first hidden layer, each of its neurons is connected to each pixel in the input image.\n",
    "<img src=\"figure.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Sequential model is a linear stack of layers.\n",
    "model = Sequential()\n",
    "\n",
    "# Adding the first hidden layer with 512 nodes. We need to tell our model the input dimension\n",
    "#in our case it is an array of length 784.\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "\n",
    "\n",
    "# An \"activation\" is just a non-linear function applied to the output\n",
    "# of the layer above. Here, we will apply the sigmoid activation \n",
    "model.add(Activation('sigmoid')) \n",
    "                              \n",
    "                           \n",
    "# Now we add the second hidden layer \n",
    "model.add(Dense(512))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# And connect it with the output layer, which has neurons equal to the number of classes\n",
    "#in our problem (10 neurons as it will predict one-hot vectors of size 10)\n",
    "model.add(Dense(10))\n",
    "\n",
    "# Finally, we will use the softmax activation\n",
    "model.add(Activation('softmax')) # \"softmax\", among other things, ensures the output \n",
    "                                 # is a valid probaility distribution, that is\n",
    "                                 # that its values are all non-negative and sum to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model\n",
    "Keras is built on top of TensorFlow or Theano, both packages that allow you to define a *computation graph* in Python, which they then compile and run efficiently on the CPU or GPU without the overhead of the Python interpreter.\n",
    "\n",
    "When compiing a model, Keras asks you to specify your **loss function**, your **optimizer** and some **metrics**. The loss function we'll use here is called *categorical crossentropy*, and is a loss function well-suited to comparing two probability distributions.\n",
    "\n",
    "Here our predictions are probability distributions across the ten different digits (e.g. \"we're 80% confident this image is a 3, 10% sure it's an 8, 5% it's a 2, etc.\"), and the target is a probability distribution with 100% for the correct category, and 0 for everything else. The cross-entropy is a measure of how different your predicted distribution is from the target distribution. [More detail at Wikipedia](https://en.wikipedia.org/wiki/Cross_entropy)\n",
    "\n",
    "The [optimizer](https://keras.io/optimizers) helps determine how quickly the model learns, how resistent it is to getting \"stuck\" or \"blowing up\". In this exercise we will use a variation of the Gradient Descent, called **Stocastic Gradient Descent (SGD)**. Instead of considering the whole training data at each iteration of the optimization, SGD only uses a batch sampled from the available training data at a time.\n",
    "\n",
    "Besides that, we can define a number of metrics (such as accuracy, mean squared error, mean absolute error etc) to follow during training. **However, by default Keras does not output the normalized accuracy!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.01) #lr = learning rate\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model!\n",
    "This is the fun part: you can feed the training data loaded in earlier into this model and it will learn to classify digits. \n",
    "\n",
    "But first, let's understand what are these arguments:\n",
    "- **X_train & Y_train:** the training data and labels;\n",
    "- **batch_size:** instead of optimizing your network on all training data, we usually optimize it over batches (small samples from the training data). In this case, we will update our weights (backpropagation) after forwarding/processing 128 images;\n",
    "- **nb_epoch:** an epoch occurs when all your training data is processed/forwarded once. We are saying we will process our data 8 times;\n",
    "- **verbose:** show loss after each epoch;\n",
    "- **class_weight:** the weight of each class, considering they are not balanced. This will ponder the loss based on how frequent each class is;\n",
    "- **validation_data:** after each epoch our model will predict these data and output the loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "          batch_size=128, epochs=8, verbose=1,\n",
    "          class_weight = train_class_weights, \n",
    "          validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, evaluate its performance on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy (NOT NORMALIZED):', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predict_classes function outputs the highest probability class\n",
    "# according to the trained classifier for each input example.\n",
    "predicted_classes = model.predict_classes(X_test)\n",
    "\n",
    "#Get the class for each one_hot array of our test samples\n",
    "Y_test_classes = np.argmax(Y_test, axis=-1)\n",
    "\n",
    "accPerClass = []\n",
    "for classIdx in range(nb_classes):\n",
    "    idx = (Y_test_classes == classIdx)\n",
    "    \n",
    "    correctPred = np.sum(predicted_classes[idx] == Y_test_classes[idx])\n",
    "    accPerClass.append( correctPred / float(test_sample_per_class[classIdx]))\n",
    "    \n",
    "print (dict(zip(range(nb_classes),accPerClass)), \"\\n\")\n",
    "print (\"Normalized Acc --> \", np.mean(accPerClass))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the output\n",
    "\n",
    "It's always a good idea to inspect the output and make sure everything looks sane. Here we'll look at some examples it gets right, and some examples it gets wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predict_classes function outputs the highest probability class\n",
    "# according to the trained classifier for each input example.\n",
    "predicted_classes = model.predict_classes(X_test)\n",
    "\n",
    "# Check which items we got right / wrong\n",
    "correct_indices = np.nonzero(predicted_classes == y_test)[0]\n",
    "incorrect_indices = np.nonzero(predicted_classes != y_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i, correct in enumerate(correct_indices[:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_test[correct].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct], y_test[correct]))\n",
    "    \n",
    "plt.figure()\n",
    "for i, incorrect in enumerate(incorrect_indices[:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[incorrect], y_test[incorrect]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of other great examples at the Keras homepage at http://keras.io and in the source code at https://github.com/fchollet/keras. Besides those, this notebook was adapted from [this](https://github.com/wxs/keras-mnist-tutorial/blob/master/MNIST%20in%20Keras.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
