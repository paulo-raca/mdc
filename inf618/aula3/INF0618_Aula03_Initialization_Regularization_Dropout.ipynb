{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization / Optimizers / Regularization / Dropout\n",
    "\n",
    "Besides the architecture, other aspects have deep impact in the network's performance. In this exercise, we will examine how weights initialization, regularization, dropout and the optimizers influence our training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility methods\n",
    "First let's import some prerequisites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import seed, sample\n",
    "seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (15,15) # Make the figures a bit bigger\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras import optimizers, regularizers, initializers\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "        \n",
    "        \n",
    "def plot_data(X, y, figsize=None):\n",
    "    if not figsize:\n",
    "        figsize = (8, 6)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(X[y==0, 0], X[y==0, 1], 'or', alpha=0.5, label=0)\n",
    "    plt.plot(X[y==1, 0], X[y==1, 1], 'ob', alpha=0.5, label=1)\n",
    "    plt.xlim((min(X[:, 0])-0.1, max(X[:, 0])+0.1))\n",
    "    plt.ylim((min(X[:, 1])-0.1, max(X[:, 1])+0.1))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    " \n",
    "def make_synth_dataset():\n",
    "    X, Y = make_classification(n_samples = 150, n_features=2, n_redundant=0, n_informative=2, class_sep=2, random_state=20)\n",
    "    testIdx = sample(list(range(150)), 50)\n",
    "    \n",
    "    X_test = np.array([X[i] for i in testIdx])\n",
    "    Y_test = np.array([Y[i] for i in testIdx])\n",
    "    \n",
    "    X_train = np.array([X[i] for i in list(range(150)) if i not in testIdx])\n",
    "    Y_train = np.array([Y[i] for i in list(range(150)) if i not in testIdx])\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "    \n",
    "def make_synth_dataset_with_noise():\n",
    "    X, Y = make_classification(n_samples = 150, n_features=2, n_redundant=0, n_informative=2, class_sep=2, random_state=20)\n",
    "    testIdx = sample(list(range(150)), 50)\n",
    "    \n",
    "    X_test = np.array([X[i] for i in testIdx])\n",
    "    Y_test = np.array([Y[i] for i in testIdx])\n",
    "    \n",
    "    X_train = np.array([X[i] for i in list(range(150)) if i not in testIdx])\n",
    "    Y_train = np.array([Y[i] for i in list(range(150)) if i not in testIdx])\n",
    "\n",
    "    noise_X = [ (1.5,-0.5), (1.6,-0.5),(1.4,-0.5), (1.5,-0.4),(1.5,-0.6)]\n",
    "    noise_Y = [0 for sample in noise_X]\n",
    "\n",
    "    X_train = np.concatenate((X_train, noise_X), axis=0)\n",
    "    Y_train = np.concatenate((Y_train, noise_Y), axis=0)\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "    \n",
    "\n",
    "def plot_decision_boundary(func, X, y, title, figsize=(9, 6)):\n",
    "    amin, bmin = X.min(axis=0) - 0.1\n",
    "    amax, bmax = X.max(axis=0) + 0.1\n",
    "    hticks = np.linspace(amin, amax, 101)\n",
    "    vticks = np.linspace(bmin, bmax, 101)\n",
    "    \n",
    "    aa, bb = np.meshgrid(hticks, vticks)\n",
    "    ab = np.c_[aa.ravel(), bb.ravel()]\n",
    "    c = func(ab)\n",
    "    cc = c.reshape(aa.shape)\n",
    "\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    contour = plt.contourf(aa, bb, cc, cmap=cm, alpha=0.8)\n",
    "    \n",
    "    ax_c = fig.colorbar(contour)\n",
    "    ax_c.set_label(\"$P(y = 1)$\")\n",
    "    ax_c.set_ticks([0, 0.25, 0.5, 0.75, 1])\n",
    "    \n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright)\n",
    "    plt.xlim(amin, amax)\n",
    "    plt.ylim(bmin, bmax)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_loss_accuracy(history, legend, plotSet=\"both\"):\n",
    "    linestyles = ['-', '--', ':', '-.']\n",
    "\n",
    "    #Plot Loss\n",
    "    plt.subplot(2, 1, 1)\n",
    "\n",
    "    for idx,hist in enumerate(historyList):\n",
    "        if plotSet in [\"train\", \"both\"]:\n",
    "            plt.plot(hist.history['loss'], linestyle=linestyles[idx%len(linestyles)])\n",
    "        if 'val_loss' in hist.history.keys() and plotSet in [\"test\", \"both\"]:\n",
    "            plt.plot(hist.history['val_loss'], linestyle=linestyles[idx%len(linestyles)])\n",
    "\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title(\"Loss\")    \n",
    "    plt.legend(legend, loc='best')    \n",
    "\n",
    "    #Plot ACC\n",
    "    plt.subplot(2, 1, 2)\n",
    "    for idx,hist in enumerate(historyList):    \n",
    "        if plotSet in [\"train\", \"both\"]:\n",
    "            plt.plot(hist.history['acc'], linestyle=linestyles[idx%len(linestyles)])\n",
    "        if 'val_acc' in hist.history.keys() and plotSet in [\"test\", \"both\"]:\n",
    "            plt.plot(hist.history['val_acc'], linestyle=linestyles[idx%len(linestyles)])\n",
    "\n",
    "    plt.ylabel('ACC')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title(\"ACC\")   \n",
    "    plt.legend(legend, loc='best')    \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = make_synth_dataset()\n",
    "X_reg_train, Y_reg_train, X_reg_test, Y_reg_test = make_synth_dataset_with_noise()\n",
    "\n",
    "print(\"Training Data without noise\")\n",
    "plot_data(X_train, Y_train)\n",
    "print(\"Testing Data\")\n",
    "plot_data(X_test, Y_test)\n",
    "print(\"Training Data with noise\")\n",
    "plot_data(X_reg_train, Y_reg_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "# Weights Initialization\n",
    "Depending on how you choose to initialize the network's weights, the network may converge faster/slower or not converge at all.\n",
    "\n",
    "Keras has a number of [initializers available](https://keras.io/initializers), from random initialization to, one of the most used, Xavier initialization (`glorot_uniform`).\n",
    "\n",
    "The default initialization in Keras is `glorot_uniform` for weights and `zeros` for the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyList = []\n",
    "initTypes = [\"zeros\", \"random_normal\", \"glorot_uniform\"]\n",
    "for initialization in initTypes:\n",
    "    #Network definition\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_shape=(2,), activation='tanh', kernel_initializer = initialization))\n",
    "    model.add(Dense(64, activation='tanh', kernel_initializer = initialization))\n",
    "    model.add(Dense(64, activation='tanh', kernel_initializer = initialization))\n",
    "    model.add(Dense(64, activation='tanh', kernel_initializer = initialization))\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer = initialization))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    historyList.append(model.fit(X_train, Y_train, epochs=200, verbose=0, validation_data=(X_test, Y_test)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend = [\"[TRAIN] zeros\", \"[TEST] zeros\", \n",
    "          \"[TRAIN] random_normal\", \"[TEST] random_normal\",\n",
    "          \"[TRAIN] glorot_uniform\", \"[TEST] glorot_uniform\"]\n",
    "plot_loss_accuracy(historyList, legend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "# Optimizers\n",
    "[Available optimizers implemented in Keras.](https://keras.io/optimizers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyList = []\n",
    "optimizers = [\"sgd\", \"adam\",\"adagrad\", \"rmsprop\"]\n",
    "for opt in optimizers:\n",
    "    #Network definition\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_shape=(2,), activation='tanh'))\n",
    "    model.add(Dense(20, activation='tanh'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    historyList.append(model.fit(X_train, Y_train, epochs=20, verbose=0, validation_data=(X_test, Y_test)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend = [\"[TRAIN] SGD\", \"[TRAIN] ADAM\", \"[TRAIN] AdaGrad\", \"[TRAIN] RMSProp\"]\n",
    "plot_loss_accuracy(historyList, legend, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend = [\"[TEST] SGD\", \"[TEST] ADAM\", \"[TEST] AdaGrad\", \"[TEST] RMSProp\"]\n",
    "plot_loss_accuracy(historyList, legend, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "# Regularization (L2)\n",
    "Keras documentation on [Regularizers](https://keras.io/regularizers/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = [\"No Reg\", \"L2(0.001)\", \"L2(0.01)\", \"L2(1.0)\", \"L2(1.5)\"]\n",
    "regularization = [None, regularizers.l2(0.001), regularizers.l2(0.01), regularizers.l2(1.0), regularizers.l2(1.5)]\n",
    "for i in list(range(len(regularization))):\n",
    "    # Make model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_shape=(2,), activation='tanh', kernel_regularizer = regularization[i]))\n",
    "    model.add(Dense(64, activation='tanh', kernel_regularizer = regularization[i]))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    from keras.optimizers import Adam\n",
    "\n",
    "    model.compile(Adam(lr=0.01), 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_reg_train, Y_reg_train, verbose=0, epochs=100)\n",
    "    plot_decision_boundary(lambda x: model.predict(x), X_reg_train, Y_reg_train, title[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "# Dropout\n",
    "[Keras documentation of the Dropout layer](https://keras.io/layers/core/#dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropoutRate = [0, 0.25, 0.5, 0.75, 0.9]\n",
    "for rate in dropoutRate:\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_shape=(2,), activation='tanh'))\n",
    "    model.add(Dense(64, activation='tanh'))\n",
    "    model.add(Dropout(rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    from keras.optimizers import Adam\n",
    "\n",
    "    model.compile(Adam(lr=0.01), 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_reg_train, Y_reg_train, verbose=0, epochs=100)\n",
    "    plot_decision_boundary(lambda x: model.predict(x), X_reg_train, Y_reg_train, \"Dropout Rate = \" + str(rate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
