{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "Data augmentation is a technique to artificially generate more data by transforming the available training samples, for example, by translation, rotation, changing brightness etc. It is specially useful when our training set is small and your model requires more data in order to learn properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "First let's import some prerequisites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import seed\n",
    "seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.rcParams['figure.figsize'] = (15,15) # Make the figures a bit bigger\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "\n",
    "#Keras class to augment data\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare data\n",
    "We will use CIFAR-10 dataset, but we will simulate a scenario where we lack training data. To do this, instead of using the usual train/val/test splits, we will have 10k images for training, 10k for validation and 40k for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "(testVal_data, testVal_label), (X_train, y_train) = cifar10.load_data()\n",
    "\n",
    "#Split trainVal data into train and val sets (already balanced)\n",
    "splitIdx = StratifiedShuffleSplit(testVal_label, 1, test_size=0.8, random_state=0)\n",
    "for val_index, test_index in splitIdx:\n",
    "    X_val, X_test = testVal_data[val_index], testVal_data[test_index]\n",
    "    y_val, y_test = testVal_label[val_index], testVal_label[test_index]\n",
    "    \n",
    "#Transform data\n",
    "X_train = X_train.reshape(X_train.shape[0], 32, 32, 3)\n",
    "X_val = X_val.reshape(X_val.shape[0], 32, 32, 3)\n",
    "X_test = X_test.reshape(X_test.shape[0], 32, 32, 3)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "X_val /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(\"Training matrix shape\", X_train.shape, \"<----- Small training set\")\n",
    "print(\"Validation matrix shape\", X_val.shape)\n",
    "print(\"Testing matrix shape\", X_test.shape)\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_val = np_utils.to_categorical(y_val, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some examples of the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_train[i])\n",
    "    plt.title(\"Class {}\".format(y_train[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "We will use one of the architectures used in an earlier example. It has only a single convolutional layer with 20 5x5 filters, a max pooling and a fully-connected layer with 10 neurons (the same size as the number of classes in CIFAR-10).\n",
    "\n",
    "We will define two equal models, in which we will train one with data augmentation and another without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelList = [Sequential(), Sequential()]\n",
    "for i in list(range(2)):\n",
    "    # Define your model here\n",
    "    model = modelList[i]\n",
    "\n",
    "    #Conv layer with 20 filters of size 5x5 and ReLU activation\n",
    "    model.add(Conv2D(20, kernel_size=(5, 5),\n",
    "    activation='relu', #ReLU activation\n",
    "    input_shape=(32,32,3))) #no need to include the batch size\n",
    "    \n",
    "    #Max pooling of size 2x2\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    #Flatten operation\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #FC layer with 10 neurons and softmax activation\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    #Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    \n",
    "    modelList.append(model)\n",
    "\n",
    "modelWithDA = modelList[0]\n",
    "modelWithoutDA = modelList[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "Keras make the class [ImageDataGenerator](https://keras.io/preprocessing/image/#imagedatagenerator-class) available to augment data. It allows a wide range of transformations on the input data and, in this exercise, we will **flip the image horizontally**, **rotate the image by a few degrees**, **translate the image in both directions by at most 1% of the height or width**.\n",
    "\n",
    "At each batch, the generator will apply these transformations with random parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataGen = ImageDataGenerator(horizontal_flip=True, \n",
    "                                  zoom_range=0.01, \n",
    "                                  width_shift_range=0.01, \n",
    "                                  height_shift_range=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model with and without Data Augmentation!\n",
    "We will train for 10 epochs (it is a small number for the sake of demonstration, but you should try more later) with batches of 128 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----> Fitting without Data Augmentation\")\n",
    "# fits the model without data augmentation\n",
    "modelWithoutDA.fit(X_train, Y_train, batch_size=128, epochs=10,\n",
    "                  validation_data=(X_val, Y_val))\n",
    "\n",
    "print(\"\\n\\n-----> Fitting with Data Augmentation\")\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "modelWithDA.fit_generator(trainDataGen.flow(X_train, Y_train, batch_size=128), \n",
    "                          steps_per_epoch=len(X_train) / 128, epochs=10,\n",
    "                          validation_data=(X_val, Y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, evaluate its performance on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreWithourDA = modelWithoutDA.evaluate(X_test,Y_test,verbose=1)\n",
    "print('[Without DA]Test loss:', scoreWithourDA[0])\n",
    "print('[Without DA]Test accuracy (NORMALIZED):', scoreWithourDA[1])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "scoreWithDA = modelWithDA.evaluate(X_test,Y_test,verbose=1)\n",
    "print('[DA]Test loss:', scoreWithDA[0])\n",
    "print('[DA]Test accuracy (NORMALIZED):', scoreWithDA[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early Stopping\n",
    "Keras has a lot of [callbacks](https://keras.io/callbacks/), which are functions that can be applied at given stages of the training procedure. One of them implements an [early stopping](https://keras.io/callbacks/#earlystopping) mechanism.\n",
    "\n",
    "It monitors a metric you chose, e.g., loss on the validation set, and if it does not improve for a number of epochs, the training is stopped. An example of usage would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "#model = Sequential()\n",
    "#model.add(...)\n",
    "\n",
    "# define early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_acc', #metric to monitor\n",
    "                          min_delta=0.0001,  #difference considered as improvement\n",
    "                          patience=5) #number of epochs with no improvement after which training will be stopped \n",
    "\n",
    "callbacks_list = [earlystop]\n",
    "\n",
    "# train the model\n",
    "model_info = model.fit(X_train, Y_train, batch_size=128, epochs=100, \n",
    "                       callbacks=callbacks_list)  # <---- pass the list of callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ImageDataGenerator](https://keras.io/preprocessing/image/#imagedatagenerator-class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
